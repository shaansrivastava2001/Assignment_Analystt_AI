# -*- coding: utf-8 -*-
"""Assignment_AnalysttAI_Part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16bUrYAhcmudC148YqKnN0UN4XCzt99p3
"""

from google.colab import files

uploaded = files.upload()

"""**Writing product urls for each product list page**"""

import requests
from bs4 import BeautifulSoup

url_file = open('page_urls.txt','r')
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'
}

c = 1
with open('all_products_urls.txt', 'w') as f:
  for i in url_file:
    url = i
    print("Link",c,"scanned!!")
    c+=1
    page = requests.get(url, headers = headers)

    htmlContent = page.content
    soup = BeautifulSoup(htmlContent,'html.parser')

    links = soup.find_all('a',{'class':'a-link-normal s-underline-text s-underline-link-text s-link-style'})
    for i in links:
      if(i.get('href')[-7:-2]=='Revie'):
        url = 'https://www.amazon.in'+i.get('href')[:-16]
        f.write(url)
        f.write('\n')

"""**Functions to extract Product name, price and other details**"""

import requests
from bs4 import BeautifulSoup

def product_name(soup):
  title = soup.find("span", attrs={"id":'productTitle'})
  if title is not None:
    ret = title.string.strip()
    return ret

  return ""

def product_price(soup):
  price = soup.find("span", attrs={"class":'a-price-whole'})
  if price is not None:
    ret = 'Rs. '+price.text.strip()[:-1]
    return ret
  
  return ""

def product_rating(soup):
  rating = soup.find("i",attrs={"class":'a-icon-star'}).find('span')
  if rating is not None:
    ret = rating.text
    return ret

  return ""

def product_reviews(soup):
  reviews = soup.find("span",attrs={"id":'acrCustomerReviewText'})
  if reviews is not None:
    ret = reviews.text.split()[0]
    return ret

  return ""

"""**Finally Writing the product details in a CSV file**"""

import csv  

header = ['Product Title', 'Price', 'Rating', 'Reviews','Product URL']

file = open('all_products_urls.txt','r')
with open('products_data.csv', 'w') as f:
  c = 0
  for i in file:
    c+=1
    print("Product",c)
    product_url = i
    page = requests.get(product_url, headers = headers)
    htmlContent = page.content
    soup = BeautifulSoup(htmlContent,'html.parser')

    name = product_name(soup)
    price = product_price(soup)
    rating = product_rating(soup)
    reviews = product_reviews(soup)
    data = [name, price, rating, reviews, product_url]
    writer = csv.writer(f)
    if(c==1):
      writer.writerow(header)
    
    writer.writerow(data)

